{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of production line with defects - Training Via Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a Anomalib model using the Anomalib API and our own dataset. This notebook is also part of the Dobot series notebooks.\n",
    "\n",
    "### Use case\n",
    "\n",
    "Using the [Dobot Magician](https://www.dobot.cc/dobot-magician/product-overview.html) we could simulate a production line system. Imagine we have a cubes factory and they need to know when a defect piece appear in the process. We know very well what is the aspecto of the normal cubes. Defects are coming no often and we need to put those defect cubes out of the production line.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10940214/174126337-b344bbdc-6343-4d85-93e8-0cb1bf39a4e3.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "| Class | Yellow cube | Red cube | Green cube | Inferencing using Anomalib\n",
    "| --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| Normal | <img src=\"https://user-images.githubusercontent.com/10940214/174083561-38eec918-efc2-4ceb-99b1-bbb4c91396b2.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083638-85ff889c-6222-4428-9c7d-9ad62bd15afe.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083707-364177d4-373b-4891-96ce-3e5ea923e440.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174129305-03d9b71c-dfd9-492f-b42e-01c5c24171cc.jpg\" alt=\"drawing\" style=\"width:150px;\"/> |\n",
    "| Abnormal | <img src=\"https://user-images.githubusercontent.com/10940214/174083805-df0a0b03-58c7-4ba8-af50-fd94d3a13e58.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083873-22699523-22b4-4a55-a3da-6520095af8af.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083944-38d5a6f4-f647-455b-ba4e-69482dfa3562.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174129253-f7a567d0-84f7-4050-8065-f00ba8bb973d.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | \n",
    "\n",
    "Using Anomalib we are expecting to see this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from git.repo import Repo\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "if current_directory.parent.name == \"500_use_cases\" and current_directory.name == \"dobot\":\n",
    "    # On the assumption that, the notebook is located in\n",
    "    #   ~/anomalib/notebooks/500_use_cases/dobot\n",
    "    root_directory = current_directory.parent.parent.parent\n",
    "elif current_directory.name == \"anomalib\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory\n",
    "else:\n",
    "    # Otherwise, we'll need to clone the anomalib repo to the `current_directory`\n",
    "    repo = Repo.clone_from(url=\"https://github.com/openvinotoolkit/anomalib.git\", to_path=current_directory)\n",
    "    root_directory = current_directory / \"anomalib\"\n",
    "\n",
    "os.chdir(root_directory)\n",
    "notebook_path = root_directory / \"notebooks\" / \"500_use_cases\" / \"dobot\"\n",
    "dataset_path = root_directory / \"datasets\" / \"cubes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data.utils import read_image\n",
    "from anomalib.deploy import OpenVINOInferencer\n",
    "from anomalib.pre_processing.transforms import Denormalize\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration - Via file\n",
    "\n",
    "The config file contains the setup of different Anomalib modules, data manager, model manager, experiment manager and callback manager. We can modify the setup modifying the configuration file and also API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Currently, there are **13** anomaly detection models available in `anomalib` library. Namely,\n",
    "\n",
    "- [CFA](https://arxiv.org/abs/2206.04325)\n",
    "- [CS-Flow](https://arxiv.org/abs/2110.02855v1)\n",
    "- [CFlow](https://arxiv.org/pdf/2107.12571v1.pdf)\n",
    "- [DFKDE](https://github.com/openvinotoolkit/anomalib/tree/main/anomalib/models/dfkde)\n",
    "- [DFM](https://arxiv.org/pdf/1909.11786.pdf)\n",
    "- [DRAEM](https://arxiv.org/abs/2108.07610)\n",
    "- [FastFlow](https://arxiv.org/abs/2111.07677)\n",
    "- [Ganomaly](https://arxiv.org/abs/1805.06725)\n",
    "- [Padim](https://arxiv.org/pdf/2011.08785.pdf)\n",
    "- [Patchcore](https://arxiv.org/pdf/2106.08265.pdf)\n",
    "- [Reverse Distillation](https://arxiv.org/abs/2201.10703)\n",
    "- [R-KDE](https://ieeexplore.ieee.org/document/8999287)\n",
    "- [STFPM](https://arxiv.org/pdf/2103.04257.pdf)\n",
    "\n",
    "In this tutorial, we'll be using Padim. Now, let's get their config paths from the respected folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"padim\"  # 'padim', 'cflow', 'stfpm', 'ganomaly', 'dfkde', 'patchcore'\n",
    "CONFIG_PATH = root_directory / \"notebooks/500_use_cases/dobot/cubes_config.yaml\"\n",
    "with open(file=CONFIG_PATH, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use [get_configurable_parameter](https://github.com/openvinotoolkit/anomalib/blob/development/anomalib/config/config.py#L114) function to read the configs from the path and return them in a dictionary. We use the default config file that comes with Padim implementation, which uses `./datasets/cubes` as the path to the dataset. We need to overwrite this after loading the config.\n",
    "\n",
    "Now, the config file is updated as we want. We can now start model training with it. Here we will be using datamodule, model and callbacks to train the model. Callbacks are self-contained objects, which contains non-essential logic. This way we could inject as many callbacks as possible such as ModelLoading, Timer, Metrics, Normalization and Visualization\n",
    "\n",
    "In addition to the training, we would like to perform inference using OpenVINO. Therefore we will set the export configuration to openvino so that anomalib would export the trained model to the openvino format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the config file to model, logger, callbacks and datamodule\n",
    "config = get_configurable_parameters(config_path=CONFIG_PATH)\n",
    "config[\"dataset\"][\"path\"] = str(dataset_path)     # or wherever the Custom Dataset is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Cubes\n",
    "\n",
    "Prepare your own dataset for normal and defect pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image'])\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.folder import Folder\n",
    "from anomalib.data.task_type import TaskType\n",
    "\n",
    "\n",
    "datamodule = Folder(\n",
    "    root=dataset_path,\n",
    "    normal_dir=\"normal\",\n",
    "    abnormal_dir=\"abnormal\",\n",
    "    normal_split_ratio=0.2,\n",
    "    image_size=(256, 256),\n",
    "    train_batch_size=32,\n",
    "    eval_batch_size=32,\n",
    "    task=TaskType.CLASSIFICATION,\n",
    ")\n",
    "datamodule.setup()          # Split the data to train/val/test/prediction sets.\n",
    "datamodule.prepare_data()   # Create train/val/test/predic dataloaders\n",
    "\n",
    "i, data = next(enumerate(datamodule.val_dataloader()))\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image size\n",
    "print(data[\"image\"].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model \n",
    "We will use Padim model for this use case, which could imported from `anomalib.models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: DeprecationWarning: From v0.10 an `'Binary*'`, `'Multiclass*', `'Multilabel*'` version now exist of each classification metric. Moving forward we recommend using these versions. This base metric will still work as it did prior to v0.10 until v0.11. From v0.11 the `task` argument introduced in this metric will be required and the general order of arguments may change, such that this metric will just function as an single entrypoint to calling the three specialized versions.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "from anomalib.models import Padim\n",
    "\n",
    "\n",
    "model = Padim(\n",
    "    input_size=(256, 256),\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\"layer1\", \"layer2\", \"layer3\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Callbacks\n",
    "\n",
    "To train the model properly, we will to add some other \"non-essential\" logic such as saving the weights, early-stopping, normalizing the anomaly scores and visualizing the input/output images. To achieve these we use `Callbacks`. Anomalib has its own callbacks and also supports PyTorch Lightning's native callbacks. So, let's create the list of callbacks we want to execute during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.post_processing import NormalizationMethod, ThresholdMethod\n",
    "from anomalib.utils.callbacks import (\n",
    "    MetricsConfigurationCallback,\n",
    "    PostProcessingConfigurationCallback,\n",
    ")\n",
    "from anomalib.utils.callbacks.export import ExportCallback, ExportMode\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    MetricsConfigurationCallback(\n",
    "        task=TaskType.CLASSIFICATION,\n",
    "        image_metrics=[\"AUROC\"],\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        mode=\"max\",\n",
    "        monitor=\"image_AUROC\",\n",
    "    ),\n",
    "    PostProcessingConfigurationCallback(\n",
    "        normalization_method=NormalizationMethod.MIN_MAX,\n",
    "        threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "    ),\n",
    "    ExportCallback(\n",
    "        input_size=(256, 256),\n",
    "        dirpath=str(notebook_path),\n",
    "        filename=\"model\",\n",
    "        export_mode=ExportMode.OPENVINO,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we set up the datamodule, model and the callbacks, we could now train the model.\n",
    "\n",
    "The final component to train the model is `pytorch_lightning` `Trainer` object, which handles train/test/predict pipeline. Let's create the trainer object to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: DeprecationWarning: From v0.10 an `'Binary*'`, `'Multiclass*', `'Multilabel*'` version now exist of each classification metric. Moving forward we recommend using these versions. This base metric will still work as it did prior to v0.10 until v0.11. From v0.11 the `task` argument introduced in this metric will be required and the general order of arguments may change, such that this metric will just function as an single entrypoint to calling the three specialized versions.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | image_threshold | AnomalyScoreThreshold    | 0     \n",
      "1 | pixel_threshold | AnomalyScoreThreshold    | 0     \n",
      "2 | model           | PadimModel               | 2.8 M \n",
      "3 | image_metrics   | AnomalibMetricCollection | 0     \n",
      "4 | pixel_metrics   | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.131    Total estimated model params size (MB)\n",
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it, loss=nan, v_num=14, image_AUROC=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: DeprecationWarning: `torchmetrics.functional.auc` has been move to `torchmetrics.utilities.compute` in v0.10 and will be removed in v0.11.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/sakcay/projects/anomalib/notebooks/500_use_cases/dobot/exported_model/openvino/model.onnx\n",
      "\t- Path for generated IR: \t/home/sakcay/projects/anomalib/notebooks/500_use_cases/dobot/exported_model/openvino\n",
      "\t- IR output name: \tmodel\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \t/home/sakcay/.pyenv/versions/3.8.13/envs/anomalib/lib/python3.8/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/sakcay/projects/anomalib/notebooks/500_use_cases/dobot/exported_model/openvino/model.xml\n",
      "[ SUCCESS ] BIN file: /home/sakcay/projects/anomalib/notebooks/500_use_cases/dobot/exported_model/openvino/model.bin\n",
      "[ SUCCESS ] Total execution time: 1.13 seconds. \n",
      "[ SUCCESS ] Memory consumed: 558 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n",
      "Epoch 0: 100%|██████████| 5/5 [00:10<00:00,  2.13s/it, loss=nan, v_num=14, image_AUROC=1.000]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  callbacks=callbacks,\n",
    "  accelerator= \"auto\",\n",
    "  auto_scale_batch_size= False,\n",
    "  check_val_every_n_epoch= 1,\n",
    "  devices= 1,\n",
    "  gpus= None,\n",
    "  max_epochs= 1,\n",
    "  num_sanity_val_steps= 0,\n",
    "  val_check_interval= 1.0,\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.47it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       image_AUROC                  1.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "test_results = trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Inference\n",
    "\n",
    "Now that we trained and tested a model, we could check a single inference result using OpenVINO inferencer object. This will demonstrate how a trained model could be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Test Image\n",
    "Let's read an image from the test set and perform inference using OpenVINO inferencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = root_directory / \"datasets/cubes/abnormal/input_20230210134059.jpg\"\n",
    "image = read_image(path=\"datasets/cubes/abnormal/input_20230210134059.jpg\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the OpenVINO Model\n",
    "\n",
    "By default, the output files are saved into `results` directory. Let's check where the OpenVINO model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(config[\"project\"][\"path\"])\n",
    "print(output_path)\n",
    "openvino_model_path = output_path / \"openvino\" / \"model.bin\"\n",
    "metadata_path = output_path / \"openvino\" / \"meta_data.json\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    config=CONFIG_PATH,  # Pass the config file to the inferencer.\n",
    "    path=openvino_model_path,  # Path to the OpenVINO IR model.\n",
    "    meta_data_path=metadata_path,  # Path to the metadata file.\n",
    "    device=\"CPU\",  # We would like to run it on an Intel CPU.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference\n",
    "Predicting an image using OpenVINO inferencer is as simple as calling `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "predictions = inferencer.predict(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `predictions` contain any relevant information regarding the task type. For example, predictions for a segmentation model could contain image, anomaly maps, predicted scores, labels or masks.\n",
    "### Visualizing Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "  \n",
    "# setting values to rows and column variables\n",
    "rows = 1\n",
    "columns = 5\n",
    "  \n",
    "# Adds a subplot at the 1st position\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(predictions.image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original\")\n",
    "  \n",
    "# Adds a subplot at the 2nd position\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(predictions.anomaly_map)\n",
    "plt.axis('off')\n",
    "plt.title(\"Anomaly Map\")\n",
    "  \n",
    "# Adds a subplot at the 3rd position\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(predictions.heat_map)\n",
    "plt.axis('off')\n",
    "plt.title(\"Heat Map\")\n",
    "  \n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(predictions.pred_mask)\n",
    "plt.axis('off')\n",
    "plt.title(\"Pred. Mask\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(predictions.segmentations)\n",
    "plt.axis('off')\n",
    "plt.title(\"Result\")\n",
    "\n",
    "print(predictions.pred_score, predictions.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
