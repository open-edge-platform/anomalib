{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `CSV` for Custom Datasets\n",
    "\n",
    "# Installing Anomalib\n",
    "\n",
    "The easiest way to install anomalib is to use pip. You can install it from the command line using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anomalib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Dataset Directory\n",
    "\n",
    "This cell ensures we change the directory to have access to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# NOTE: Provide the path to the dataset root directory.\n",
    "#   If the dataset is not downloaded, it will be downloaded\n",
    "#   to this directory.\n",
    "dataset_root = Path.cwd().parent.parent / \"datasets\" / \"hazelnut_toy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CSV Dataset (for Custom Datasets) via API\n",
    "\n",
    "Here we show how to utilize custom datasets to train anomalib models using the CSV datamodule. The CSV datamodule allows for more flexible dataset organization, where image paths and labels are specified in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image\n",
    "\n",
    "from anomalib import TaskType\n",
    "from anomalib.data import CSV, CSVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CSV file for the dataset\n",
    "\n",
    "First, let's create a CSV file that contains the image paths and labels for our hazelnut dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_file(dataset_path: Path, csv_filename: str = \"hazelnut_dataset.csv\") -> Path:\n",
    "    \"\"\"Create a CSV file from the hazelnut dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Path to the hazelnut dataset.\n",
    "        csv_filename (str, optional): Name of the CSV file.\n",
    "            Defaults to \"hazelnut_dataset.csv\".\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the created CSV file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for category in [\"good\", \"crack\"]:\n",
    "        image_dir = dataset_path / category\n",
    "        for image_path in image_dir.glob(\"*.jpg\"):\n",
    "            mask_path = dataset_path / \"mask\" / category / image_path.name if category != \"good\" else \"\"\n",
    "            data.append(\n",
    "                {\n",
    "                    \"image_path\": str(image_path),\n",
    "                    \"label\": \"normal\" if category == \"good\" else \"abnormal\",\n",
    "                    \"mask_path\": str(mask_path) if mask_path else \"\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    csv_path = Path(csv_filename)\n",
    "    data_frame.to_csv(csv_path, index=False)\n",
    "    return csv_path\n",
    "\n",
    "\n",
    "csv_file_path = create_csv_file(dataset_root)\n",
    "print(f\"CSV file created at: {csv_file_path}\")\n",
    "\n",
    "# Display the first few rows of the CSV file\n",
    "pd.read_csv(csv_file_path).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule\n",
    "\n",
    "Now that we have created a CSV file for our dataset, let's create an Anomalib datamodule using the CSV class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_datamodule = CSV(\n",
    "    name=\"hazelnut_toy\",\n",
    "    csv_path=csv_file_path,\n",
    "    task=TaskType.SEGMENTATION,\n",
    ")\n",
    "csv_datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(csv_datamodule.test_dataloader()))\n",
    "print(\"Test data:\", data.image.shape, data.gt_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset\n",
    "\n",
    "We can also create a standalone PyTorch dataset instance using the CSVDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVDataset??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a transform that resizes the input image to 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "transform = Resize(image_size, antialias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataset_classification = CSVDataset(\n",
    "    name=\"hazelnut_toy\",\n",
    "    csv_path=csv_file_path,\n",
    "    transform=transform,\n",
    "    task=\"classification\",  # or TaskType.CLASSIFICATION,\n",
    ")\n",
    "csv_dataset_classification.samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv_dataset_classification[0]\n",
    "print(data.image_path, data.image.shape, data.gt_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataset_segmentation = CSVDataset(\n",
    "    name=\"hazelnut_toy\",\n",
    "    csv_path=csv_file_path,\n",
    "    transform=transform,\n",
    "    task=\"segmentation\",  # TaskType.SEGMENTATION,\n",
    ")\n",
    "csv_dataset_segmentation.samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv_dataset_segmentation[0]  # Choose an abnormal sample\n",
    "print(data.image_path, data.mask_path, data.image.shape, data.gt_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the image and the mask..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a CSV dataset with train, validation, and test splits. For more details, you could check out the documentation [here](https://anomalib.readthedocs.io/en/v1.0.1/markdown/guides/reference/data/image/csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_image(data.image.clone())\n",
    "msk = to_pil_image(data.gt_mask.float() * 255).convert(\"RGB\")\n",
    "\n",
    "Image.fromarray(np.hstack((np.array(img), np.array(msk))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
