{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalib Models\n",
    "In this notebook, we show how anomalib models could be initialized via Python API. As shown in [README.md](https://github.com/openvinotoolkit/anomalib#training), following models are supported in anomalib:\n",
    "\n",
    "- [CFlow](anomalib/models/cflow)\n",
    "- [DFKDE](anomalib/models/dfkde)\n",
    "- [DFM](anomalib/models/dfm)\n",
    "- [Draem](anomalib/models/draem)\n",
    "- [FastFlow](anomalib/models/fastflow)\n",
    "- [GANomaly](anomalib/models/ganomaly)\n",
    "- [PADIM](anomalib/models/padim)\n",
    "- [PatchCore](anomalib/models/patchcore)\n",
    "- [Reverse Distillation](anomalib/models/reverse_distillation)\n",
    "- [STFPM](anomalib/models/stfpm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer\n",
    "from pathlib import Path\n",
    "from types import MethodType\n",
    "from typing import Any\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from anomalib.data.mvtec import MVTec\n",
    "from anomalib.models.fastflow.lightning_model import Fastflow\n",
    "from anomalib.models.fastflow.torch_model import FastflowModel\n",
    "from anomalib.models.patchcore import Patchcore\n",
    "from anomalib.utils.callbacks import (\n",
    "    MetricsConfigurationCallback,\n",
    "    MinMaxNormalizationCallback,\n",
    "    VisualizerCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module\n",
    "To train each model end-to-end, we do need to have a dataset. In our [previous notebooks](https://github.com/openvinotoolkit/anomalib/tree/development/notebooks/100_datamodules), we demonstrate how to initialize benchmark and custom datasets. In this tutorial, we will use MVTec AD DataModule. We assume that `datasets` directory is created in the `anomalib` root directory and `MVTec` dataset is located in `datasets` directory. Let's confirm this with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sakcay/projects/anomalib/notebooks/200_models')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sakcay/projects/anomalib')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go to the main anomalib root dir\n",
    "root = Path.cwd().parent.parent\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/sakcay/projects/anomalib/datasets/BTech'),\n",
       " PosixPath('/home/sakcay/projects/anomalib/datasets/MVTec'),\n",
       " PosixPath('/home/sakcay/projects/anomalib/datasets/bottle'),\n",
       " PosixPath('/home/sakcay/projects/anomalib/datasets/README.txt'),\n",
       " PosixPath('/home/sakcay/projects/anomalib/datasets/hazelnut_toy')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((root / \"datasets\").iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we checked we have `datasets` directory and `MVTec` is already located in `datasets`, we could create the datamodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVTec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32, 256, 256]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule = MVTec(\n",
    "    root=\"../../datasets/MVTec/\",\n",
    "    category=\"bottle\",\n",
    "    image_size=256,\n",
    "    train_batch_size=32,\n",
    "    test_batch_size=32,\n",
    "    num_workers=8,\n",
    "    task=\"segmentation\",\n",
    ")\n",
    "datamodule.setup()\n",
    "i, data = next(enumerate(datamodule.test_dataloader()))\n",
    "data[\"image\"].shape, data[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Now that we have created MVTec datamodule, we could create the models. We could start with `PatchCore` since it is the ranked #1 model on the MVTec AD category on papers with code. \n",
    "\n",
    "Each model on anomalib has the following structure:\n",
    "```\n",
    "        anomalib/models/<model_name>\n",
    "        ├── README.md           # Readme file containing description and benchmarks.\n",
    "        ├── __init__.py         # Model initialization.\n",
    "        ├── config.yaml         # Stores the model configurations.  \n",
    "        ├── torch_model.py      # Torch model implementing the basic forward-pass mechanism.\n",
    "        ├── anomaly_map.py      # [Optional] module generating anomaly heatmaps.      \n",
    "        ├── lightning_model.py  # Lightning module implementing training mechanism\n",
    "        └── loss.py             # [Optional] module implementing loss computation. \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastflowModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = FastflowModel(input_size=[256, 256], backbone=\"resnet18\", flow_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, `torch_model` implements the basic forward-pass mechanism of the model for both `train` and `test` phases. In `train` phase, the model returns the training output such as feature maps. During the `test` phase, it returns the anomaly heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.training = True\n",
    "train_output = torch_model(data[\"image\"])\n",
    "hidden_variables, log_jacobian = train_output\n",
    "hidden_variables[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.training = False\n",
    "anomaly_map = torch_model(data[\"image\"])\n",
    "anomaly_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when the `training` is `False`, it means the model is in val/test/inference stage, which produces the `anomaly_map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module\n",
    "The main module for each anomalib models is the `LightningModule` that stores the torch_model as its attribute and sorts out the train-test mechanism. Let's see how the LightningModule of `Fastflow` model is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fastflow??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fastflow(\n",
    "    input_size=[256, 256],\n",
    "    backbone=\"resnet18\",\n",
    "    flow_steps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training = True\n",
    "train_output = model(data[\"image\"])\n",
    "hidden_variables, log_jacobian = train_output\n",
    "hidden_variables[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the Lightning Module also returns the same output as `torch_model`. This is because it stores `torch_model` as its attribute and uses it in its `forward` method. Therefore, it is possible to call the forward-pass with `model(x)`. For the implementational details, you could refer to [this](https://github.com/openvinotoolkit/anomalib/blob/development/anomalib/models/components/base/anomaly_module.py#L64) link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `torch_model`, `lightning_module` also produces an `anomaly_map` when the `training` is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.training = False\n",
    "anomaly_map = model(data[\"image\"])\n",
    "anomaly_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we see how anomalib models are structured, we could try to train a model. In order to properly train the models we need to set number of callbacks such as model saving and early stopping.\n",
    "\n",
    "Let's create these callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now that we created the datamodule, let's create the model. We could use Patchcore model as it currently achieves the highest performance on MVTec dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(lightning_module: LightningModule, optimizer: Optimizer) -> Any:\n",
    "    \"\"\"Override to customize the :meth:`~pytorch_lightning.core.lightning.LightningModule.configure_optimizers`\n",
    "    method.\n",
    "\n",
    "    Args:\n",
    "        lightning_module: A reference to the model.\n",
    "        optimizer: The optimizer.\n",
    "        lr_scheduler: The learning rate scheduler (if used).\n",
    "    \"\"\"\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "optimizer = Adam(params=model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-5)\n",
    "\n",
    "fn = partial(configure_optimizers, optimizer=optimizer)\n",
    "# update_wrapper(fn, configure_optimizers)  # necessary for `is_overridden`\n",
    "\n",
    "model.configure_optimizers = MethodType(fn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function configure_optimizers at 0x7ff5ae9b3c10>, optimizer=Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 1e-05\n",
       "))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 1e-05\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Patchcore(\n",
    "    input_size=(224, 224),\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\"layer3\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(),\n",
    "    MetricsConfigurationCallback(adaptive_threshold=True),\n",
    "    MinMaxNormalizationCallback(),\n",
    "    VisualizerCallback(task=\"segmentation\", log_images_to=[\"local\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final component that we need to train a model is `pytorch_lightning` `Trainer` object, which handles train/test/predict pipeline. Let's create the trainer object to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"auto\", # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "    check_val_every_n_epoch=1, # Don't validate before extracting features.,\n",
    "    devices=1,\n",
    "    gpus=1, # Set automatically,\n",
    "    gradient_clip_val=0,\n",
    "    limit_predict_batches=1.0,\n",
    "    limit_test_batches=1.0,\n",
    "    limit_train_batches=1.0,\n",
    "    limit_val_batches=1.0,\n",
    "    log_every_n_steps=50,\n",
    "    max_epochs=1,\n",
    "    max_steps=-1,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    overfit_batches=0.0,\n",
    "    precision=32,\n",
    "    reload_dataloaders_every_n_epochs=0,\n",
    "    track_grad_norm=-1,\n",
    "    val_check_interval=1.0, # Don't validate before extracting features.                  ,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer` object has number of options that suit all specific needs. For more details, refer to [Lightning Documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) to see how it could be tweaked to your needs.\n",
    "\n",
    "Let's train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | image_threshold       | AdaptiveThreshold        | 0     \n",
      "1 | pixel_threshold       | AdaptiveThreshold        | 0     \n",
      "2 | training_distribution | AnomalyScoreDistribution | 0     \n",
      "3 | min_max               | MinMax                   | 0     \n",
      "4 | model                 | PatchcoreModel           | 11.7 M\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "46.758    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, loss=nan, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(datamodule=datamodule, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('anomalib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f26beec5b578f06009232863ae217b956681fd13da2e828fa5a0ecf8cf2ccd29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
